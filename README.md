# A Compendium of Artificial Intelligence (specifically for lawyers)
This compendium was produced for the 2023 Texas Advanced Business Law Conference.  This document is seen as far more useful than a traditional paper.  Moreover, this compendium may be updated in the future, in stark contrast to traditional papers.

## Table of Contents

1.  [Why Lawyers Should Care About AI](https://github.com/ronchi/ai_compendium/blob/main/README.md#why-lawyers-should-care-about-ai)
1.  [What is Artificial Intelligence?](https://github.com/ronchi/ai_compendium/blob/main/README.md#what-is-artificial-intelligence)
2.  [Types of AI](https://github.com/ronchi/ai_compendium/blob/main/README.md#types-of-ai)
    1.  [Natural Language Processing](https://github.com/ronchi/ai_compendium/blob/main/README.md#natural-language-processing-nlp)
    2.  [Generative AI (like ChatGPT)](https://github.com/ronchi/ai_compendium/blob/main/README.md#generative-ai-chatgpt-like-combination-of-answering-questions-and-text-generation)
3.  [AI for the Practice of Law](https://github.com/ronchi/ai_compendium/blob/main/README.md#ai-for-the-practice-of-law)
4.  A Few Things That Litigators Should Know About AI
5.  A Few Things That Transactional Attorneys Should Know About AI
6.  Regulation and Governance of AI

## Why Lawyers Should Care About AI

1.    Go over what our profession is about (knowledge for, and acting on behalf of clients)
2.    Go over how AI has elements of agency (but not regulated by a Bar)

- Roberto Legaspi, Zhengqi He, and Taro Toyoizumi, "[Synthetic agency: sense of agency in artificial intelligence](https://doi.org/10.1016/j.cobeha.2019.04.004)", Current Opinion in Behavioral Sciences, Vol. 29, October 2019, Pages 84-90.  (From the Abstract: "The concept of sense of agency (SoA) has garnered considerable attention in human science at least in the past two decades. Coincidentally, about two decades ago, artificial intelligence (AI) research witnessed an explosion of proposed theories on agency mostly based on dynamical approaches. However, despite this early burst of enthusiasm, SoA models in AI remain limited. We review the state of AI research on SoA, seen predominantly in developmental robotics, vis-à-vis the psychology and neurocognitive treatments, and examine how AI can further achieve stronger SoA models. We posit that AI is now poised to better inform SoA given its advances on self-attribution of action–outcome effects, action selection, and Bayesian inferencing, and argue that synthetic agency has never been more compelling.") 

## What is Artificial Intelligence?
There is no one single (adequate) definition of artificial intelligence.  Fortunately, there is a "classic" formulation of AI in the categorization of thinking and acting, as illustrated in the following set of quotes, courtesy of AI pioneers Stuart Russell and Peter Norvig in their book "[Artificial Intelligence: A Modern Approach](https://people.eecs.berkeley.edu/~russell/aima/)" (4th ed., 2023):

### Thinking Humanly: The Cognitive Modeling Approach

`"The exciting new effort to make computers think ... machines with minds, in the full and literal sense." (Gaugeland, 1985)`

`"{The automation of} Activities that we associate with human thinking, activities such as decision-making, problem solving, learning..." (Bellman, 1978)`

### Thinking Rationally: The "Laws of Thought" Approach

`"The study of mental faculties through the use of computational models." (Charniak and McDermott, 1985)`

`"The study of the computations that make it possible to perceive, reason, and act." (Winston, 1992)`

### Acting Humanly: The Turing Test Approach

`"The art of creating machines that perform functions that require intelligence when performed by people." (Kurzweil, 1990)`

`"The study of how to make computers do things at which, at the moment, people are better." (Rich and Knight, 1991)`

### Acting Rationally: The Rational Agent Approach

`"Computational Intelligence is the study of the design of intelligent agents." (Poole et al., 1998)`

`AI ...is concerned with intelligent behavior in artificats." (Nilsson, 1998)`

You can find other definitions [here](https://www.britannica.com/technology/artificial-intelligence), [here](https://www.hpe.com/us/en/what-is/artificial-intelligence.html), and [here](https://www.ibm.com/topics/artificial-intelligence).

A few things to note.  All of those quotes were from the Twentieth Century.  AI was first conceived by [Aristotle](https://en.wikipedia.org/wiki/Aristotle).  Work on neural networks started in the 1950's.  What you see now is the culmination of decades of work by thousands of people.

## Types of AI

Based on the definition outline above (Thinking/Acting & Humanly/Rationally) you could derive a chart such as the following:

![Types of AI](https://github.com/ronchi/ai_compendium/blob/main/Alternate_AI_types.png)

However, it is often more practical to group the types of AI into categories that resemble the products and services with which we are accustomed or encounter regularly.  Some types (or combinations of types) of AI are famous.  Many types of AI labor in the background -- unseen and unnoticed -- yet some of which have significant legal implications.

The most important thing to remember is that there are many types of AI.  One set of types has to do with the four categories above (thinking/acting humanly/rationally).  A more practical set of categories can be distilled from the products that we encounter in our daily lives, as outlined in the chart below:

![Types of AI](https://github.com/ronchi/ai_compendium/blob/main/Standard_AI_types.png)

We will address each type of AI, as well as link to various other sources, below.

### Natural Language Processing (NLP)

NLP is the aspect of AI that lawyers deal with most often -- both directly and indirectly.

#### Definition of Natural Language Processing

#### Context Extraction

##### Use Cases

##### Examples

##### In General:

##### Legal Aspects:

##### Roll Your Own

#### Classification

##### Use Cases

##### Examples

##### In General:

##### Legal Aspects:

##### Roll Your Own

#### Machine Translation

##### Use Cases

##### Examples

##### In General:

##### Legal Aspects:

##### Roll Your Own

#### Answering Questions

##### Use Cases

##### Examples

##### In General:

##### Legal Aspects:

##### Roll Your Own

#### Text Generation

##### Use Cases

##### Examples

##### In General:

##### Legal Aspects:

##### Roll Your Own

#### Generative AI (ChatGPT-like combination of Answering Questions and Text Generation)

What is Generative AI?

Generative AI is a subset of Natural Language Processing.  Specifically, Generative AI employs the *Answering Questions* and *Text Generation* aspects of NLP.  The *Answering Questions* aspect is the interface that is used to ask the question.  In recent parlance, the *Answering Questions* aspect is called a *Prompt*.  The answer that comes back is from the *Text Generation* machine.  A classic example of the Promt/Generation duo is [ChatGPT](https://openai.com/blog/chatgpt).

Essentially, Generative AI makes use of a model (such as a "Large Language Model" like GPT4) that enables the AI to generate new content with little human interaction.  You should think of it as an automated paraphraser, albeit with some serious caveats.  AI professionals refer to Generative AI as "a stochastic parrot" or "a reality-starved paraphraser".  The AI doesn't know what it is saying (or whether what it says is real or not), but it says it very nicely.  Sometimes, the Generative AI can depart significantly from reality, and this is known as "going [hysterical](https://www.windowscentral.com/software-apps/users-show-microsoft-bings-chatgpt-ai-going-off-the-deep-end)."  However, even with those limitations, Generative AI has met with great commercial success.  Since most Generative AI is not tied to reality, marketing people love it.  

Beware, however, for lawyers have been sanctioned for using Generative AI unwisely.  See, *e.g.*, [Mata v. Avianca, Inc.](https://storage.courtlistener.com/recap/gov.uscourts.nysd.575368/gov.uscourts.nysd.575368.54.0_3.pdf) ("Peter LoDuca, Steven A. Schwartz and the law firm of Levidow, Levidow & Oberman P.C. (the "Levidow Firm") (collectively, "Respondents") abandoned their responsibilities when they submitted non-existent judicial opinions with fake quotes and citations created by the artificial intelligence tool ChatGPT, then continued to stand by the fake opinions after judicial orders called their existence into question.")

Unwise reliance on Generative AI is enough of a problem that some courts now have standing orders related to AI-generated material.  *See, e.g.*, an [order](http://chrome-extension/efaidnbmnnnibpcajpcglclefindmkaj/https:/www.ilnd.uscourts.gov/_assets/_documents/_forms/_judges/Fuentes/Standing%20Order%20For%20Civil%20Cases%20Before%20Judge%20Fuentes%20rev'd%205-31-23.pdf) by Magistrate Judge Gabriel A. Fuentes (N.D. Ill.) that was adopted on May 31, 2023.

Having said all of this, you should know that you can "fine tune" your own AI on something real.  In other words, you can direct the AI to look at a corpus of documents (e.g., opinions from the Texas Supreme Court, or [ESI](https://en.wikipedia.org/wiki/Electronically_stored_information_(Federal_Rules_of_Civil_Procedure)) from a witness) and let it answer questions -- and identify the document(s) from which it drew to answer your question.  Such fine tuning can reduce the amount of hysteria produced by the model and, in some cases, cause the model to tell you from whence it derived the answer -- which makes it easier to verify the results.

Here is a set of articles that discuss Generative AI in depth.

##### In General:

- [Gartner Experts Answer the Top Generative AI Questions for Your Enterprise](https://www.gartner.com/en/topics/generative-ai) (This article discusses what Generative AI is, the history, the hype, the reality, and the use cases.)
- [Generative AI: Perspectives from Stanford HAI](https://hai.stanford.edu/sites/default/files/2023-03/Generative_AI_HAI_Perspectives.pdf) (This March, 2023 paper is from the Stanford University Human-Centered Artificial Intelligence initiative discusses the potential of Generative AI.)

##### Legal Aspects:

- [The Generative AI Revolution: Key Legal Considerations for the Consumer Products Industry](https://www.natlawreview.com/article/generative-ai-revolution-key-legal-considerations-consumer-products-industry) (This article discusses the important legal aspects of generative AI systems such as [OpenAI's ChatGPT](https://openai.com/blog/chatgpt), [Microsoft's Copilot](https://adoption.microsoft.com/en-us/copilot/), and similar commercial offerings.)
- [ChatGPT Goes to Law School](https://ssrn.com/abstract=4335905) (The authors of this paper used ChatGPT to generate answers to four real exams at the University of Minnesota Law School.  The resulting answers were graded (blindly) and ChatGPT received an average grade of C+ -- low, but passing.  Further advise was provided on how to use ChatGPT in legal writing.)
- [Generative AI in the Legal Profession](https://clp.law.harvard.edu/knowledge-hub/magazine/issues/generative-ai-in-the-legal-profession/) ("When Open AI released ChatGPT in late 2022, it galvanized the collective imagination—and collective anxiety. Even lawyers wondered how such new AI technologies would change their profession.")
- [The Implications of ChatGPT for Legal Services and Society](https://clp.law.harvard.edu/article/the-implications-of-chatgpt-for-legal-services-and-society/) ("To demonstrate ChatGPT's remarkable sophistication and potential implications, for both legal services and society more generally, most of this paper was generated in about an hour through prompts within the chatbot. The disruptions from AI’s rapid development are no longer in the distant future. They have arrived, and this essay offers a small taste of what lies ahead.")

##### Rolling Your Own:

- [A Complete Guide to Fine Tuning Large Language Models](https://www.simform.com/blog/completeguide-finetuning-llm/) ("Fine-tuning in large language models (LLMs) involves re-training pre-trained models on specific datasets, allowing the model to adapt to the specific context of your business needs. This process can help you create highly accurate language models, tailored to your specific business use cases.")
- [LLM Finetuning](https://huggingface.co/docs/autotrain/llm_finetuning) (discusses how to fine tune your large language model (LLM) with your own data.)
- [Build LLM-powered chatbot in 5 minutes using HugChat and Streamlit](https://dev.to/codemaker2015/build-llm-powered-chatbot-in-5-minutes-using-hugchat-and-streamlit-2f53) ("We will dive into a step-by-step process of developing an LLM-powered chatbot using HugChat, a powerful Python library that simplifies the integration of LLMs into chatbot applications. Furthermore, we will leverage Streamlit, a user-friendly framework for creating interactive web applications, to provide a seamless user interface and deployment platform for our chatbot.")
- [How to use HuggingChat (free ChatGPT) in Python](https://medium.com/cassandra-cryptoassets/how-to-use-huggingchat-free-chatgpt-in-python-ac51ed9dd1f) (A step by step guide to using the HuggingChat Python API.)
- [How To Build Your Own Custom ChatGPT With Custom Knowledge Base](https://betterprogramming.pub/how-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e) (How to feed your own ChatGPT bot with custom data sources, such as certain court opinions, custodian's ESI, etc.)
- [Step-By-Step Guide to Building a Chatbot Knowledge Base](https://www.userlike.com/en/blog/chatbot-knowledge-base) ("Without data, AI chatbots are just a pretty face with nothing upstairs. An important first step in using automation and natural language processing to help your customers is to connect your chatbot to your knowledge base.")
- [How To Create A Knowledge Base For Your Chatbot](https://itchronicles.com/customer-service/how-to-create-a-knowledge-base-for-your-chatbot) (If you are making a chatbot to interact with your clients (for such tasks as basic information intake, answering common questions, etc.), then this article is for you.)
- [How to Train an AI Chatbot With Custom Knowledge Base Using ChatGPT API](https://beebom.com/how-train-ai-chatbot-custom-knowledge-base-chatgpt-api/) (This is a step-by-step guide for general users to create a specialized version of ChatGPT that is more specific to your practice.  Note, this methodology works specifically with ChatGPT, so you'll need an account with OpenAI in order to implement the technique disclosed in the article.)
- [Talk to Claude](https://claude.ai/login).  Claude2 is a large language model that can accommodate much larger documents for discussion purposes.  For example, ChatGPT limits you to about 3,000 words (about 4,000 tokens) in the question.  Claude2 on the other hand, allows for up to 100,000 tokens (about 75,000 words) so you're able to incorporate large contracts, deposition transcripts, etc. and ask questions about those documents specifically.  Claude2 is also able to ingest normal court opinions, so Claude2 is particularly better suited to legal research.  See, *e.g.*, Jeremy Caplan [Meet Claude: A helpful new AI assistant](https://wondertools.substack.com/p/claude), [Wonder Tools Blog](https://wondertools.substack.com/) (July 20, 2023).

## AI for the Practice of Law

- [Generative AI in the Legal Profession](https://clp.law.harvard.edu/knowledge-hub/magazine/issues/generative-ai-in-the-legal-profession/) ("When Open AI released ChatGPT in late 2022, it galvanized the collective imagination—and collective anxiety. Even lawyers wondered how such new AI technologies would change their profession.")
- [The Implications of ChatGPT for Legal Services and Society](https://clp.law.harvard.edu/article/the-implications-of-chatgpt-for-legal-services-and-society/) ("To demonstrate ChatGPT's remarkable sophistication and potential implications, for both legal services and society more generally, most of this paper was generated in about an hour through prompts within the chatbot. The disruptions from AI’s rapid development are no longer in the distant future. They have arrived, and this essay offers a small taste of what lies ahead.")
- [Generative Legal Minds](https://clp.law.harvard.edu/knowledge-hub/magazine/issues/generative-ai-in-the-legal-profession/generative-legal-minds/) ("How ChatGPT and other technologies might change legal research and writing")
- Liepia, R., Sartor, G. & Wyner, A. (2019). [Arguing about causes in law: a semi-formal framework for causal arguments](http://cronfa.swan.ac.uk/Record/cronfa49685) *Artificial Intelligence and Law* (2019). ("Disputes over causes play a central role in legal argumenta- tion and liability attribution. Legal approaches to causation often strug- gle to capture cause-in-fact in complex situations, e.g. overdetermination, preemption, omission. In this paper, we first assess three current theo- ries of causation (but-for, NESS, ‘actual causation’) to illustrate their strengths and weaknesses in capturing cause-in-fact. Secondly, we intro- duce a semi-formal framework for modelling causal arguments through strict and defeasible rules. Thirdly, the framework is applied to the Al- then vaccine injury case. And lastly, we discuss the need for new crite- ria based on a common causal argumentation framework and propose ideas on how to integrate the current theories of causation to assess the strength of causal arguments, while also acknowledging the tension be- tween evidence-based and policy-based causal analysis in law.")
- Saskia van de Ven, Rinke Hoekstra, Joost Breuker, Lars Wortel, and Abdallah El-Ali, "[Judging Amy: Automated Legal Assessment using OWL 2](https://www.researchgate.net/publication/221218532_Judging_Amy_Automated_Legal_Assessment_using_OWL_2)" (This paper discusses using AI (in the form of an Ontology of legal concepts) as a form of artificial judge.  From the Abstract:  "One of the most salient tasks in law is legal assessment, and concerns the problem of determining whether some case is allowed or disallowed given an appropriate body of legal norms. In this paper we describe a system and Prot ́eg ́e 4 plugin, called OWL Judge, that uses standard OWL 2 DL reasoning for legal assessment. Norms are represented in terms of the LKIF Core ontology, as generic situation descriptions in which something (state, action) is deemed obliged, prohibited or permitted. We demonstrate the design patterns for defining the norms and actual cases. Furthermore we show how a DL classifier can be used to assess individual cases and automatically generate a lex specialis exception structure using OWL Judge. We illustrate our approach with a worked-out example of university library regulations.")
- Adam Wyner, "[A Legal Case OWL Ontology with an Instantiation of Popov v. Hayashi](https://www.researchgate.net/publication/228566250_A_Legal_Case_OWL_Ontology_with_an_Instantiation_of_Popov_v_Hayashi)", *The Knowledge Engineering Review*, (January, 2010).  ("The paper provides an OWL ontology for legal cases with an instantiation of the legal case Popov v. Hayashi. The ontology makes explicit the conceptual knowledge of the legal case domain, supports reasoning about the domain, and can be used to annotate the text of cases, which in turn can be used to populate the ontology. A populated ontology is a case base which can be used for information retrieval, information extraction, and case based reasoning. The ontology contains not only elements of indexing the case (e.g. the parties, jurisdiction, and date), but as well elements used to reason to a decision such as argument schemes and the components input to the schemes. We use the Prote ́ge ́ ontology editor and knowledge acquisition system, current guidelines for ontology development, and tools for visual and linguistic presentation of the ontology.")

### Machine Learning

#### Deep Learning

#### Supervised Learning

#### Unsupervised Learning

### Vision

#### Image Recognition

#### Machine Vision

### Speech

#### Speech-to-Text

#### Text-to-Speech

### Expert Systems

### Genetic Algorithms

### Robotics

### Knowledge, Reasoning, and Planning

- Shulayeva, O., Siddharthan, A. & Wyner, A., [Recognizing cited facts and principles in legal judgements](http://cronfa.swan.ac.uk/Record/cronfa40674), *Artificial Intelligence and Law*, 25(1), 107-126 (2017).  ("In common law jurisdictions, legal professionals cite facts and legal principles from precedent cases to support their arguments before the court for their intended outcome in a current case. This practice stems from the doctrine of stare decisis, where cases that have similar facts should receive similar decisions with respect to the principles. It is essential for legal professionals to identify such facts and principles in precedent cases, though this is a highly time intensive task. In this paper, we present studies that demonstrate that human annotators can achieve reasonable agreement on which sentences in legal judgements contain cited facts and principles (respectively, j 1⁄4 0:65 and j 1⁄4 0:95 for inter- and intra-annotator agreement). We further demonstrate that it is feasible to automatically annotate sentences containing such legal facts and principles in a supervised machine learning framework based on linguistic features, reporting per category precision and recall figures of between 0.79 and 0.89 for classifying sentences in legal judgements as cited facts, principles or neither using a Bayesian classifier, with an overall j of 0.72 with the human-annotated gold standard.")
- Wyner, A., Bench-Capon, T., Dunne, P. & Cerutti, F, "[Senses of ‘argument’ in instantiated argumentation frameworks](http://cronfa.swan.ac.uk/Record/cronfa40673)" *Argument & Computation*, 6(1), 50-72 (2015). (From the Abstract: "Abstract Argumentation Frameworks (AFs) provide a fruitful basis for exploring issues of defeasible reasoning. Their power largely derives from the abstract nature of the arguments within the framework, where arguments are atomic nodes in an undifferentiated relation of attack. This abstraction conceals different senses of argument, namely a single-step reason to a claim, a series of reasoning steps to a single claim, and reasoning steps for and against a claim. Concrete instantiations encounter difficulties and complexities as a result of conflating these senses. To distinguish them, we provide an approach to instantiating AFs in which the nodes are restricted to literals and rules, encoding the underlying theory directly. Arguments in these senses emerge from this framework as distinctive structures of nodes and paths. As a consequence of the approach, we reduce the effort of computing argumentation extensions, which is in contrast to other approaches. Our framework retains the theoretical and computational benefits of an abstract AF, distinguishes senses of argument, and efficiently computes extensions. Given the mixed intended audience of the paper, the style of presentation is semi-formal.")

## A Few Things That Litigators Should Know About AI

- Grimm, et al., "[Artificial Intelligence as Evidence](https://scholarlycommons.law.northwestern.edu/njtip/vol19/iss1/2/)" *Northwestern Journal of Technology and Intellectual Property Volume 19* | Issue 1 | Article 2 (December, 2021).  (From the Abstract: "This article explores issues that govern the admissibility of Artificial Intelligence (“AI”) applications in civil and criminal cases, from the perspective of a federal trial judge and two computer scientists, one of whom also is an experienced attorney. It provides a detailed yet intelligible discussion of what AI is and how it works, a history of its development, and a description of the wide variety of functions that it is designed to accomplish, stressing that AI applications are ubiquitous, both in the private and public sectors. Applications today include: health care, education, employment-related decision-making, finance, law enforcement, and the legal profession. The article underscores the importance of determining the validity of an AI application (i.e., how accurately the AI measures, classifies, or predicts what it is designed to), as well as its reliability (i.e., the consistency with which the AI produces accurate results when applied to the same or substantially similar circumstances), in deciding whether it should be admitted into evidence in civil and criminal cases. The article further discusses factors that can affect the validity and reliability of AI evidence, including bias of various types, “function creep,” lack of transparency and explainability, and the sufficiency of the objective testing of AI applications before they are released for public use. The article next provides an in-depth discussion of the evidentiary principles that govern whether AI evidence should be admitted in court cases, a topic which, at present, is not the subject of comprehensive analysis in decisional law. The focus of this discussion is on providing a step-by-step analysis of the most important issues, and the factors that affect decisions on whether to admit AI evidence. Finally, the article concludes with a discussion of practical suggestions intended to assist lawyers and judges as they are called upon to introduce, object to, or decide on whether to admit AI evidence.")
- Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, Hannaneh Hajishirzi, "[SELF-INSTRUCT: Aligning Language Models with Self-Generated Instructions](https://arxiv.org/abs/2212.10560)" *Arxiv.org* (May, 2023).  (AI that instructs itself!  From the Abstract: "Large “instruction-tuned” language models (i.e., finetuned to respond to instructions) have demonstrated a remarkable ability to gener- alize zero-shot to new tasks. Nevertheless, they depend heavily on human-written instruction data that is often limited in quantity, diversity, and creativity, therefore hindering the generality of the tuned model. We introduce SELF-INSTRUCT, a framework for improving the instruction-following capabilities of pre-trained language models by bootstrapping off their own generations. Our pipeline generates instructions, input, and output samples from a language model, then filters invalid or similar ones before using them to finetune the original model. Applying our method to the vanilla GPT3, we demonstrate a 33% absolute improvement over the original model on SUPER-NATURALINSTRUCTIONS, on par with the performance of InstructGPT001 which was trained with private user data and human annotations. For further evaluation, we curate a set of expert-written instructions for novel tasks, and show through human evaluation that tuning GPT3 with SELF-INSTRUCT outperforms using existing public instruction datasets by a large margin, leaving only a 5% absolute gap behind InstructGPT001. SELF-INSTRUCT provides an almost annotation-free method for aligning pre-trained language models with instructions, and we release our large synthetic dataset to facilitate future studies on instruction tuning."  Incidentally, the code for SELF-INSTRUCT (and associated data) is available [here](https://github.com/yizhongw/self-instruct).)

## A Few Things that Transactional Attorneys Should Know About AI

## Regulation and Governance of AI

- Mark Chinen, "[The need for the international governance of AI](https://www.researchgate.net/publication/371339740_The_need_for_the_international_governance_of_AI)" *The International Governance of Artificial Intelligence* (pp. 8-33, (Chapter 1)).  ("...[A]rtificial intelligence stands to impact every domain of human life, and several of these impacts will be international in scope. This raises the question whether it is desirable or possible to govern AI applications at the international level. This chapter lays the groundwork for that inquiry. It begins with basic concepts that will be used throughout the book. It then surveys possible international impacts of AI in various domains and concludes by placing debates about the governance of artificial intelligence within larger problems with the governance of technology in general.")
- El-Mahdi El-Mhamdi, Sadegh Farhadkhani, Rachid Guerraoui, Nirupam Gupta, Lˆe-Nguyˆen Hoang, Rafa ̈el Pinot, S ́ebastien Rouault, and John Stephan, "[On the Impossible Safety of Large AI Models](https://arxiv.org/abs/2209.15259)" *Arxiv.org* (May, 2023).  ("Large AI Models (LAIMs), of which large language models are the most prominent recent example, showcase some impressive performance. However they have been empirically found to pose serious security issues. This paper systematizes our knowledge about the fundamental impossibility of building arbitrarily accurate and secure machine learning models. More precisely, we identify key challenging features of many of today’s machine learning settings. Namely, high accuracy seems to require memorizing large training datasets, which are often user-generated and highly heterogeneous, with both sensitive information and fake users. We then survey statistical lower bounds that, we argue, constitute a compelling case against the possibility of designing high-accuracy LAIMs with strong security guarantees.")
- Hannah Rose Kirk, Bertie Vidgen, Paul Röttger, Scott A. Hale, "[Personalisation within bounds: A risk taxonomy and policy framework for the alignment of large language models with personalised feedback](https://arxiv.org/abs/2303.05453)" *Arxiv.org* (March, 2023).  ("Large language models (LLMs) are used to generate content for an increasingly wide range of tasks, and are set to reach a growing audience in coming years due to integration in product interfaces like ChatGPT or search engines like Bing. This intensifies the need to ensure that models are aligned with human preferences and do not produce unsafe, inaccurate or toxic outputs. While alignment techniques like reinforcement learning with human feedback (RLHF) and red-teaming can mitigate some safety concerns and improve model capabilities, it is unlikely that an aggregate fine-tuning process can adequately represent the full range of users’ preferences and values. Different people may legitimately disagree on their preferences for language and conversational norms, as well as on values or ideologies which guide their communication. Personalising LLMs through micro-level preference learning processes may result in models that are better aligned with each user. However, there are several normative challenges in defining the bounds of a societally-acceptable and safe degree of personalisation. In this paper, we ask how, and in what ways, LLMs should be personalised. First, we review literature on current paradigms for aligning LLMs with human feedback, and identify issues including (i) a lack of clarity regarding what alignment means; (ii) a tendency of technology providers to prescribe definitions of inherently subjective preferences and values; and (iii) a “tyranny of the crowdworker”, exacerbated by a lack of documentation in who we are really aligning to. Second, we present a taxonomy of benefits and risks associated with personalised LLMs, for individuals and society at large. Finally, we propose a three-tiered policy framework that allows users to experience the benefits of personalised alignment, while restraining unsafe and undesirable LLM-behaviours within (supra-)national and organisational bounds.")


